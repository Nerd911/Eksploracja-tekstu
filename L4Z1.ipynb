{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict as dd\n",
    "from typing import List, Dict, Set\n",
    "import nltk\n",
    "from tqdm import tqdm_notebook, tnrange\n",
    "import pickle\n",
    "from termcolor import colored\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "class OrderedSet(collections.MutableSet):\n",
    "\n",
    "    def __init__(self, iterable=None):\n",
    "        self.end = end = [] \n",
    "        end += [None, end, end]         # sentinel node for doubly linked list\n",
    "        self.map = {}                   # key --> [key, prev, next]\n",
    "        if iterable is not None:\n",
    "            self |= iterable\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.map)\n",
    "\n",
    "    def __contains__(self, key):\n",
    "        return key in self.map\n",
    "\n",
    "    def add(self, key):\n",
    "        if key not in self.map:\n",
    "            end = self.end\n",
    "            curr = end[1]\n",
    "            curr[2] = end[1] = self.map[key] = [key, curr, end]\n",
    "\n",
    "    def discard(self, key):\n",
    "        if key in self.map:        \n",
    "            key, prev, next = self.map.pop(key)\n",
    "            prev[2] = next\n",
    "            next[1] = prev\n",
    "\n",
    "    def __iter__(self):\n",
    "        end = self.end\n",
    "        curr = end[2]\n",
    "        while curr is not end:\n",
    "            yield curr[0]\n",
    "            curr = curr[2]\n",
    "\n",
    "    def __reversed__(self):\n",
    "        end = self.end\n",
    "        curr = end[1]\n",
    "        while curr is not end:\n",
    "            yield curr[0]\n",
    "            curr = curr[1]\n",
    "\n",
    "    def pop(self, last=True):\n",
    "        if not self:\n",
    "            raise KeyError('set is empty')\n",
    "        key = self.end[1][0] if last else self.end[2][0]\n",
    "        self.discard(key)\n",
    "        return key\n",
    "\n",
    "    def __repr__(self):\n",
    "        if not self:\n",
    "            return '%s()' % (self.__class__.__name__,)\n",
    "        return '%s(%r)' % (self.__class__.__name__, list(self))\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, OrderedSet):\n",
    "            return len(self) == len(other) and list(self) == list(other)\n",
    "        return set(self) == set(other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(\"Dane/wikipedyjka.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lemma_mapping():\n",
    "    all_lemmas = dd(list)\n",
    "    for line in open('Dane/polimorfologik-2.1.txt', encoding='utf-8'):\n",
    "        L = line.split(';')[:2]\n",
    "        all_lemmas[L[1].lower()].append(L[0].lower())\n",
    "    return all_lemmas\n",
    "lemma_mapping = to_lemma_mapping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_list = []\n",
    "key = \"\"\n",
    "with open(\"Dane/fp_wiki.txt\") as f:\n",
    "    cntr = -1\n",
    "    for line in f:\n",
    "        tokenized_line = line.split(\": \")\n",
    "        if tokenized_line[0] == \"TITLE\":\n",
    "            if tokenized_line[-1][-1] == \"\\n\":\n",
    "                tokenized_line[-1] = tokenized_line[-1][:-1]\n",
    "            cntr+=1\n",
    "            wiki_list.append((tokenized_line[-1], []))\n",
    "            continue\n",
    "        wiki_list[-1][1].append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "cntr = 0\n",
    "positional_index = dd(list)\n",
    "articles_pos = []\n",
    "for title, article in wiki_list:\n",
    "    cntr += 1\n",
    "    articles_pos.append(cntr)\n",
    "    for line in article:\n",
    "        tokenized_line = nltk.word_tokenize(line)\n",
    "        for w in tokenized_line:\n",
    "            w = w.lower()\n",
    "            for lemma in lemma_mapping[w]:\n",
    "                positional_index[w].append(cntr)\n",
    "                positional_index[lemma].append(cntr)\n",
    "            cntr += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = conn.cursor()\n",
    "for k, v in positional_index.items():\n",
    "    if not k.isalnum():\n",
    "        continue\n",
    "    c.execute(f'''DROP TABLE IF EXISTS tab_{k}_''')\n",
    "    c.execute(f'''CREATE TABLE tab_{k}_\n",
    "             (position INTEGER)''')\n",
    "    for ind in v:\n",
    "        c.execute(f\"INSERT INTO tab_{k}_ VALUES ({ind})\")\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.execute(f'''DROP TABLE IF EXISTS articles_tab''')\n",
    "c.execute(f'''CREATE TABLE articles_tab\n",
    "             (position INTEGER)''')\n",
    "for ind in articles_pos:\n",
    "    c.execute(f\"INSERT INTO articles_tab VALUES ({ind})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_query(w, i = 0):\n",
    "    res = OrderedSet()\n",
    "    try:\n",
    "        for el in c.execute(f'SELECT * FROM tab_{w}_'):\n",
    "            res.add(el[0] - i)\n",
    "    except:\n",
    "        pass\n",
    "    for lemma in lemma_mapping[w]:\n",
    "        try:\n",
    "            for el in c.execute(f'SELECT * FROM tab_{lemma}_'):\n",
    "                res.add(el[0] - i)\n",
    "        except:\n",
    "            pass\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phrase_query(tokens):\n",
    "    res = OrderedSet()\n",
    "    for i, t in enumerate(tokens):\n",
    "        if i == 0:\n",
    "            res |= word_query(t)\n",
    "            continue\n",
    "        res &= word_query(t, i)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_search(array, target):\n",
    "    lower = 0\n",
    "    prev_lower = 0\n",
    "    upper = len(array)\n",
    "    while lower < upper:   # use < instead of <=\n",
    "        prev_lower = lower\n",
    "        x = lower + (upper - lower) // 2\n",
    "        val = array[x]\n",
    "        if target == val:\n",
    "            return x\n",
    "        elif target > val:\n",
    "            if lower == x:   # these two are the actual lines\n",
    "                break        # you're looking for\n",
    "            lower = x\n",
    "        elif target < val:\n",
    "            upper = x\n",
    "    return prev_lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_list[binary_search(articles_pos, 35048823)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lemmas(tokens):\n",
    "    res = []\n",
    "    for t in tokens:\n",
    "        t = t.lower()\n",
    "        res.append(t)\n",
    "        res += lemma_mapping[t]\n",
    "    return res\n",
    "\n",
    "def in_lemmas(w, lemmas):\n",
    "    w = w.lower()\n",
    "    if w in lemmas:\n",
    "        return True\n",
    "    for l in lemma_mapping[w]:\n",
    "        if l in lemmas:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def search_and_print(query):\n",
    "    tokens = [t.lower() for t in nltk.word_tokenize(query)]\n",
    "    article_set = phrase_query(tokens)\n",
    "    lemmas = to_lemmas(tokens)\n",
    "    articles = list(set([binary_search(articles_pos, el) for el in article_set]))\n",
    "    print(articles)\n",
    "    for el in articles:\n",
    "        title, article = wiki_list[el]\n",
    "        for w in nltk.word_tokenize(title):\n",
    "            if in_lemmas(w, lemmas):\n",
    "                print(colored(w, \"green\"), end=\" \")\n",
    "            else:\n",
    "                print(w, end=\" \")\n",
    "        print(\"\\n\")\n",
    "        for line in article:\n",
    "            for w in nltk.word_tokenize(line):\n",
    "                if in_lemmas(w, lemmas):\n",
    "                    print(colored(w, \"green\"), end=\" \")\n",
    "                else:\n",
    "                    print(w, end=\" \")\n",
    "            print(\"\")\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[901961, 364059]\n",
      "Funkcja osobliwa \n",
      "\n",
      "Funkcja osobliwa \n",
      "Funkcja osobliwa ( określana również jako ) – dowolna funkcja ƒ ( `` x `` ) , określona dla przedziału [ `` a `` , `` b `` ] , posiadająca następujące właściwości : \n",
      "Klasycznym przykładem funkcji osobliwej jest funkcja Cantora , nazywana czasami diabelskimi schodami . Istnieją jednak również inne funkcje tak nazywane . Jedna z nich jest określona przez odwzorowanie koliste . \n",
      "Jeśli ƒ ( `` x `` ) = 0 dla wszystkich `` x `` ≤ `` a `` oraz ƒ ( `` x `` ) = 1 dla wszystkich `` x `` ≥ `` b `` , to można założyć , że dana funkcja przedstawia dystrybuantę dla \u001b[32mzmiennej\u001b[0m \u001b[32mlosowej\u001b[0m , która ani nie jest cząstkową \u001b[32mzmienną\u001b[0m \u001b[32mlosową\u001b[0m ( gdyż prawdopodobieństwo wynosi zero w każdym punkcie ) ani absolutnie \u001b[32mciągłą\u001b[0m \u001b[32mzmienną\u001b[0m \u001b[32mlosową\u001b[0m ( gdyż gęstość prawdopodobieństwa jest zerowa wszędzie , gdzie jest określona ) . \n",
      "\n",
      "\n",
      "Dyskretyzacja ( statystyka ) \n",
      "\n",
      "Dyskretyzacja ( statystyka ) \n",
      "Dyskretyzacja – przekształcenie \u001b[32mciągłej\u001b[0m \u001b[32mzmiennej\u001b[0m \u001b[32mlosowej\u001b[0m w dyskretną \u001b[32mzmienną\u001b[0m \u001b[32mlosową\u001b[0m . Dyskretyzacja przekształca tym samym rozkład \u001b[32mzmiennej\u001b[0m \u001b[32mlosowej\u001b[0m z \u001b[32mciągłego\u001b[0m na dyskretny . \n",
      "W przypadku jednowymiarowym zwykle dzieli się zbiór liczb rzeczywistych lub przedział obejmujący wartości danej \u001b[32mzmiennej\u001b[0m \u001b[32mlosowej\u001b[0m na pewną liczbę rozłącznych podprzedziałów i każdemu przypisuje jakąś wartość . \n",
      "Na ogół wybiera się przekształcenie monotoniczne . Wówczas dyskretyzacja przekształca \u001b[32mzmienną\u001b[0m na skali interwałowej oraz ilorazowej na \u001b[32mzmienną\u001b[0m na skali porządkowej . \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "search_and_print(\"ciągła zmienna losowa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
