{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict as dd\n",
    "from typing import List, Dict, Set\n",
    "import nltk\n",
    "from tqdm import tqdm_notebook, tnrange\n",
    "import pickle\n",
    "from termcolor import colored\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mikol\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "class OrderedSet(collections.MutableSet):\n",
    "\n",
    "    def __init__(self, iterable=None):\n",
    "        self.end = end = [] \n",
    "        end += [None, end, end]         # sentinel node for doubly linked list\n",
    "        self.map = {}                   # key --> [key, prev, next]\n",
    "        if iterable is not None:\n",
    "            self |= iterable\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.map)\n",
    "\n",
    "    def __contains__(self, key):\n",
    "        return key in self.map\n",
    "\n",
    "    def add(self, key):\n",
    "        if key not in self.map:\n",
    "            end = self.end\n",
    "            curr = end[1]\n",
    "            curr[2] = end[1] = self.map[key] = [key, curr, end]\n",
    "\n",
    "    def discard(self, key):\n",
    "        if key in self.map:        \n",
    "            key, prev, next = self.map.pop(key)\n",
    "            prev[2] = next\n",
    "            next[1] = prev\n",
    "\n",
    "    def __iter__(self):\n",
    "        end = self.end\n",
    "        curr = end[2]\n",
    "        while curr is not end:\n",
    "            yield curr[0]\n",
    "            curr = curr[2]\n",
    "\n",
    "    def __reversed__(self):\n",
    "        end = self.end\n",
    "        curr = end[1]\n",
    "        while curr is not end:\n",
    "            yield curr[0]\n",
    "            curr = curr[1]\n",
    "\n",
    "    def pop(self, last=True):\n",
    "        if not self:\n",
    "            raise KeyError('set is empty')\n",
    "        key = self.end[1][0] if last else self.end[2][0]\n",
    "        self.discard(key)\n",
    "        return key\n",
    "\n",
    "    def __repr__(self):\n",
    "        if not self:\n",
    "            return '%s()' % (self.__class__.__name__,)\n",
    "        return '%s(%r)' % (self.__class__.__name__, list(self))\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if isinstance(other, OrderedSet):\n",
    "            return len(self) == len(other) and list(self) == list(other)\n",
    "        return set(self) == set(other)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wikipedia:\n",
    "    def __init__(self):\n",
    "        self.lemmas = self._to_lemma_mapping()\n",
    "        self.list = []\n",
    "        key = \"\"\n",
    "        with open(\"Dane/fp_wiki.txt\") as f:\n",
    "            cntr = -1\n",
    "            for line in f:\n",
    "                tokenized_line = line.split(\": \")\n",
    "                if tokenized_line[0] == \"TITLE\":\n",
    "                    if tokenized_line[-1][-1] == \"\\n\":\n",
    "                        tokenized_line[-1] = tokenized_line[-1][:-1]\n",
    "                    cntr+=1\n",
    "                    self.list.append((tokenized_line[-1], []))\n",
    "                    continue\n",
    "                self.list[-1][1].append(line)\n",
    "    \n",
    "    def _to_lemma_mapping(self) -> Dict[str, List[str]]:\n",
    "        all_lemmas = {}\n",
    "        for line in open('Dane/polimorfologik-2.1.txt', encoding='utf-8'):\n",
    "            L = line.split(';')[:2]\n",
    "            if L[1].lower() not in all_lemmas or L[0].lower() == L[1].lower():\n",
    "                all_lemmas[L[1].lower()] = L[0].lower()\n",
    "        return all_lemmas\n",
    "    \n",
    "    def _to_lemmas(self, words):\n",
    "        result = []\n",
    "        for w in words:\n",
    "            w = w.lower()\n",
    "            if w in self.lemmas:\n",
    "                result.append(self.lemmas[w])\n",
    "            else:\n",
    "                result.append(w)\n",
    "        return result\n",
    "    \n",
    "    def _load_word(self, w, alternative_load = True):\n",
    "        if not os.path.isfile(f\"Database/{w}\"):\n",
    "            return (OrderedSet(), OrderedSet(), OrderedSet(), OrderedSet())\n",
    "        if alternative_load:\n",
    "            with open( f\"Database/{w}\", \"r\") as ifile:\n",
    "                res = []\n",
    "                for line in ifile:\n",
    "                    res.append(OrderedSet([int(el) for el in line.split(\" \")]))\n",
    "                return res\n",
    "        else:\n",
    "            return pickle.load(open( f\"Database/{w}\", \"rb\"))\n",
    "        \n",
    "    def _load(self, w, alternative_load = True):    \n",
    "        res = self._load_word(w, alternative_load)\n",
    "        if w not in self.lemmas:\n",
    "            return res\n",
    "        tmp = self._load_word(self.lemmas[w], alternative_load)\n",
    "        res[1] |= tmp[0] | tmp[1]\n",
    "        res[3] |= tmp[2] | tmp[3]\n",
    "        return res\n",
    "    \n",
    "    def search(self, query):\n",
    "        tokenized_query = nltk.word_tokenize(query)\n",
    "        vbs = [self._load(t.lower()) for t in tokenized_query]\n",
    "        if len(vbs) == 0:\n",
    "            return []\n",
    "        res1 = vbs[0][0]\n",
    "        res2 = vbs[0][1] | vbs[0][0]\n",
    "        res3 = vbs[0][2] | vbs[0][1] | vbs[0][0]\n",
    "        res4 = vbs[0][3] | vbs[0][2] | vbs[0][1] | vbs[0][0]\n",
    "\n",
    "        for el in vbs:\n",
    "            res1 &= el[0]\n",
    "            res2 &= el[1] | el[0]\n",
    "            res3 &= el[2] | el[1] | el[0]\n",
    "            res4 &= el[3] | el[2] | el[1] | el[0]\n",
    "            \n",
    "        res = list(res1) + list(res2 - res1) + list(res3 - res2 - res1) + list(res4 - res3 - res2 - res1)\n",
    "        return res\n",
    "    \n",
    "    def search_and_print(self, query):\n",
    "        articles = self.search(query)\n",
    "        tokens = nltk.word_tokenize()\n",
    "        lemmas = self.to_lemmas(tokens)\n",
    "        for el in articles:\n",
    "            title, article = self.list[el]\n",
    "            for w in nltk.word_tokenize(title):\n",
    "                if w in self.lemmas and self.lemmas[w] in lemmas:\n",
    "                    print(colored(w, green), end=\" \")\n",
    "                else:\n",
    "                    print(w, end=\" \")\n",
    "            print(\"\\n\")\n",
    "            for line in articles:\n",
    "                for w in line:\n",
    "                    if w in self.lemmas and self.lemmas[w] in lemmas:\n",
    "                        print(colored(w, green), end=\" \")\n",
    "                else:\n",
    "                    print(w, end=\" \")\n",
    "                print(\"\")\n",
    "            print(\"\")\n",
    "        \n",
    "    def preprocess(self, alternative_dump = True):\n",
    "        global cache\n",
    "        titles = dd(lambda: OrderedSet())\n",
    "        titles_lemmas = dd(lambda: OrderedSet())\n",
    "        articles = dd(lambda: OrderedSet())\n",
    "        articles_lemmas = dd(lambda: OrderedSet())\n",
    "        words = set()\n",
    "        \n",
    "        for i in tnrange(len(self.list)):\n",
    "            l = self.list[i]\n",
    "            title, article = l\n",
    "            article = \"\".join(article)\n",
    "            tokens = nltk.word_tokenize(title)\n",
    "            for w in tokens:\n",
    "                w = w.lower()\n",
    "                words.add(w)\n",
    "                titles[w].add(i)\n",
    "            for w in self._to_lemmas(tokens):\n",
    "                \n",
    "                titles_lemmas[w].add(i)\n",
    "#                 print(article)\n",
    "            tokens = nltk.word_tokenize(article)\n",
    "            for w in tokens:\n",
    "                w = w.lower()\n",
    "                words.add(w)\n",
    "                articles[w].add(i)\n",
    "            for w in self._to_lemmas(tokens):\n",
    "                articles_lemmas[w].add(i)\n",
    "#         print(words)\n",
    "        cache.append(titles)\n",
    "        cache.append(titles_lemmas)\n",
    "        cache.append(articles)\n",
    "        cache.append(articles_lemmas)\n",
    "        cache.append(words)\n",
    "\n",
    "        for w in tqdm_notebook(words):\n",
    "            lemma_w = w if w not in self.lemmas else self.lemmas[w]\n",
    "            if alternative_dump:\n",
    "                with open( f\"Database/{w}\", \"w\") as ofile:\n",
    "                    for el in titles[w]:\n",
    "                        ofile.write(f\"{el} \")\n",
    "                    ofile.write(\"\\n\")\n",
    "                    for el in titles_lemmas[lemma_w]:\n",
    "                        ofile.write(f\"{el} \")\n",
    "                    ofile.write(\"\\n\")\n",
    "                    for el in articles[w]:\n",
    "                        ofile.write(f\"{el} \")\n",
    "                    ofile.write(\"\\n\")\n",
    "                    for el in articles_lemmas[w]:\n",
    "                        ofile.write(f\"{el} \")\n",
    "                    ofile.write(\"\\n\")\n",
    "                    \n",
    "            else:\n",
    "                pickle.dump((\n",
    "                    titles[w],\n",
    "                    titles_lemmas[lemma_w],\n",
    "                    articles[w],\n",
    "                    articles_lemmas[lemma_w]\n",
    "                ), open( f\"Database/{w}\", \"wb\" ))\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki = Wikipedia()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mikol\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:105: TqdmDeprecationWarning: Please use `tqdm.notebook.trange` instead of `tqdm.tnrange`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1500ccd73bda45cb90b60616b222e162",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1208362.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wiki.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
